\documentclass[15pt,margin=1in,innermargin=-4.5in,blockverticalspace=-0.25in]{tikzposter}
\geometry{paperwidth=42in,paperheight=30in}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{adjustbox}
\usepackage{enumitem}
\usepackage[backend=biber,style=numeric]{biblatex}
\usepackage{emory-theme}
\renewcommand{\familydefault}{\sfdefault}

\usepackage{mwe} % for placeholder images

\addbibresource{refs.bib}

% set theme parameters
\tikzposterlatexaffectionproofoff
\usetheme{EmoryTheme}
\usecolorstyle{EmoryStyle}

\title{\textbf{Reproduction of `Multi-Task Learning using Uncertainty to Weigh Losses'}}
\author{D. Baerg, O. Key, J. Materzynska, M. Tong}
\institute{Department of Computer Science, University of Oxford}
\titlegraphic{\qquad \qquad \qquad \qquad \qquad  \includegraphics[width=0.06\textwidth]{oxford.jpg}}

% begin document
\begin{document}
\maketitle


\centering
\begin{columns}
    \column{0.32}

    \block{A new approach to multi-task learning}{
             
        Paper on `Multi-Task Learning using Uncertainty to Weigh Losses' -- Kendall, Gal \& Cipolla, 2017.
        \begin{itemize}
        \item Principled approach to multi-task learning, demonstrated on the multi-task problem of semantic segmentation, instance segmentation and depth regression for the Cityscapes dataset
        
        \begin{tikzfigure}[Model architecture (Kendall et al., 2017)]
            \includegraphics[width=1\linewidth]{overall}
        \end{tikzfigure}
        
        \item 3 key contributions:
        \begin{enumerate}
       \item  Aleatoric homoscedastic uncertainty to weigh losses in different tasks
       \item  Unified architecture for all three tasks with the same encoder for different tasks, with a separate decoder for each task
       \item Demonstrating that loss weighting is important for performance, and showing how superior performance can be achieved on multi-task models compared to individually trained single-task models
       \end{enumerate}
       \end{itemize}
                    }
       
    
    \block{Selected reproduction objective}{
    
       \begin{itemize}
       
       \item Comparison of multi-task learning with single-task learning and learned loss weights with fixed loss weights
       \item Subsampled dataset, Tiny Cityscapes, requires less computation time
       
       \end{itemize}
    
       \begin{tikzfigure}[Results that we aimed to reproduce (Kendall et al., 2017)]
            \includegraphics[width=1\linewidth]{data.png}
        \end{tikzfigure}
        
    }
   
   \column{0.36} 
   \block{Technical details}{
       \begin{itemize}
       \item Overall joint loss function: 
       \[ \mathcal{L} \approx 
       \frac{1}{\sigma^2_\text{sem}} \mathcal{L}_\text{sem}  + \log \sigma_\text{sem}
       + \frac{1}{2\sigma^2_\text{ins}} \mathcal{L}_\text{ins}+ \log \sigma_\text{ins}
       + \frac{1}{2\sigma^2_\text{dep}} \mathcal{L}_\text{dep}
        + \log \sigma_\text{dep} 
  \]
     
  \item Model architecture
\begin{itemize}
\item DeepLabv3 encoder architecture, including ResNet-101 layers with dilated convolutions and Atrous Spatial Pooling Pyramid module to increase contextual awareness
\item Simple decoder architecture with two convolutional layers for each task
\end{itemize}
\item Sacred with MongoDB to save results
\end{itemize}
       
    }
    
    \block{Reproduction results}{

   	\begin{itemize}
   	\item Table 1 with paper hyperparams
   	\item Semseg lr search
   	\item Semseg weight decay search
   	\item ASPP dilation size search
   	\item Single batch (+ IoU calibration)
   
    \end{itemize} 
         
    }

    
    \column{0.32}
    \block{Success on other multi-task problems}{
    
    \begin{itemize}
    \item Applying the principled approach to other multi-task learning problems
    \end{itemize}
    
\textbf{MNIST and Fashion MNIST}
   
   \begin{itemize}
   
   \item Table 1 style
   \item Grid search for optimal weights on two task
   \item Uncertainty convergence from different initial values
   
   \end{itemize}   
    }
   
\end{columns}
\end{document}