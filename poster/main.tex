\documentclass[15pt,margin=1in,innermargin=-4.5in,blockverticalspace=-0.25in]{tikzposter}
\geometry{paperwidth=42in,paperheight=30in}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{adjustbox}
\usepackage{enumitem}
\usepackage[backend=biber,style=numeric]{biblatex}
\usepackage{emory-theme}
\renewcommand{\familydefault}{\sfdefault}

\usepackage{mwe} % for placeholder images

\addbibresource{refs.bib}

% set theme parameters
\tikzposterlatexaffectionproofoff
\usetheme{EmoryTheme}
\usecolorstyle{EmoryStyle}

\title{\textbf{Reproduction of `Multi-Task Learning using Uncertainty to Weigh Losses'}}
\author{D. Baerg, O. Key, J. Materzynska, M. Tong}
\institute{Department of Computer Science, University of Oxford}
\titlegraphic{\qquad \qquad \qquad \qquad \qquad  \includegraphics[width=0.06\textwidth]{oxford.jpg}}

% begin document
\begin{document}
\maketitle


\centering
\begin{columns}
    \column{0.32}

    \block{A new approach to multi-task learning}{
             
        Paper on `Multi-Task Learning using Uncertainty to Weigh Losses' -- Kendall, Gal \& Cipolla, 2017.
        \begin{itemize}
        \item Principled approach to multi-task learning, demonstrated on the multi-task problem of semantic segmentation, instance segmentation and depth regression for the Cityscapes dataset
        
        \begin{tikzfigure}[Model architecture (Kendall et al., 2017)]
            \includegraphics[width=1\linewidth]{overall}
        \end{tikzfigure}
        
        \item 3 key contributions:
        \begin{enumerate}
       \item  Aleatoric homoscedastic uncertainty to weigh losses in different tasks
       \item  Unified architecture for all three tasks with the same encoder for different tasks, with a separate decoder for each task
       \item Demonstrating that loss weighting is important for performance, and showing how superior performance can be achieved on multi-task models compared to individually trained single-task models
       \end{enumerate}
       
       \end{itemize}
 
       
    }
    \block{Achievable objectives}{
    
       \begin{itemize}
       
       \item Comparison of multi-task learning with single-task learning and learned loss weights with fixed loss weights
       \item Subsampled dataset, Tiny Cityscapes, requires less computation time
       
       \end{itemize}
    
       \begin{tikzfigure}[Results that we aimed to reproduce (Kendall et al., 2017)]
            \includegraphics[width=1\linewidth]{data.png}
        \end{tikzfigure}
        
    }
    
    \column{0.36}
    \block{Method}{
     
     Sacred with MongoDB
   	\begin{itemize}
    \item Hyperparameter optimisation 
    \end{itemize} 
         
    }
    
    \block{Results of reproduction}{
     
    }
    
    \block{Success on other multi-task problems}{
    
    \begin{itemize}
    \item Applying the principled approach to other multi-task learning problems
    \item
    \end{itemize}
    
    
   \begin{enumerate}
   
   
   \item MNIST
   
   
   
   \item Fashion MNIST
   
   
   
   \item Fashion MNIST with auto-encoder
   
   
   
   \end{enumerate}
         
  
         
         
    }

    \column{0.32}
    \block{Comparison}{
        
        }
    
    \block{Summary}{
        
    }
    
   
\end{columns}
\end{document}