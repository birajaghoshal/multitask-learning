{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "])\n",
    "train_dataset = torchvision.datasets.MNIST('~/.torch/models/mnist', train=True, download=True, \n",
    "                                           transform=transform)\n",
    "test_dataset = torchvision.datasets.MNIST('~/.torch/models/mnist', train=False, download=True, \n",
    "                                           transform=transform)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_shape(x, shape):\n",
    "    assert tuple(x.shape[-2:]) == tuple(shape), f'Expected shape ending {shape}, got {x.shape}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, size: (int, int)):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=25, kernel_size=12, padding=0, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=25, out_channels=64, kernel_size=5, padding=2)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        assert_shape(x, self.size)\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        assert_shape(x, (9, 9)) # Should this be the same size?\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "        assert_shape(x, (9, 9))\n",
    "        \n",
    "        x = F.max_pool2d(x, 2)\n",
    "        assert_shape(x, (4, 4))\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features=4*4*64, out_features=1024)\n",
    "        self.fc2 = nn.Linear(in_features=1024, out_features=num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        assert_shape(x, (4, 4))\n",
    "        \n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "class Model(nn.Module):\n",
    "    def __init__(self, size: (int, int), num_classes: int):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.encoder = Encoder(size)\n",
    "        self.decoder1 = Decoder(num_classes=3)\n",
    "        self.decoder2 = Decoder(num_classes=10)\n",
    "        \n",
    "        self.weight1 = nn.Parameter(torch.tensor([1.0]))\n",
    "        self.weight2 = nn.Parameter(torch.tensor([1.0]))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        assert_shape(x, self.size)\n",
    "        \n",
    "        x = self.encoder(x)\n",
    "        x1 = self.decoder1(x)\n",
    "        x2 = self.decoder2(x)\n",
    "        return x1, x2\n",
    "        \n",
    "model = Model((28,28), 10)\n",
    "model = model.cuda()\n",
    "for image, labels in train_dataloader:\n",
    "    image = image.cuda()\n",
    "    image /= 255\n",
    "    model(image)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 0, 2, 2, 2, 1, 2, 2, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def labels_to_1(labels):\n",
    "    \"\"\"Task 1 is 3, 7 or other\"\"\"\n",
    "    # 2 is class other.\n",
    "    converted = torch.full_like(labels, 2)\n",
    "    converted[labels == 3] = 0\n",
    "    converted[labels == 7] = 1\n",
    "    return converted\n",
    "\n",
    "labels_to_1(torch.tensor([1,2,3,4,5,6,7,8,9,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion1 = nn.CrossEntropyLoss()\n",
    "criterion2 = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "enable1 = True\n",
    "enable2 = True\n",
    "learn_weights = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: -1.44 (-2.5295610427856445, -1.9533275365829468)\n",
      "Epoch 1: -1.86 (-3.035012722015381, -2.3334403038024902)\n",
      "Epoch 2: -2.14 (-3.371398687362671, -2.6007096767425537)\n",
      "Epoch 3: -2.4 (-3.6361169815063477, -2.827561855316162)\n",
      "Epoch 4: -2.61 (-3.863328456878662, -3.0023446083068848)\n",
      "Epoch 5: -2.86 (-4.093160629272461, -3.2284069061279297)\n",
      "Epoch 6: -3.0 (-4.2603936195373535, -3.388504981994629)\n",
      "Epoch 7: -3.18 (-4.3957905769348145, -3.574328660964966)\n",
      "Epoch 8: -3.32 (-4.539775371551514, -3.7055885791778564)\n",
      "Epoch 9: -3.57 (-4.725478649139404, -3.9001665115356445)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    epoch_loss = 0\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        image, labels = data\n",
    "        \n",
    "        image = image.cuda()\n",
    "        labels = labels.cuda()\n",
    "        \n",
    "        image /= 255\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output1, output2 = model(image)\n",
    "        \n",
    "        if enable1:\n",
    "            loss1 = criterion1(output1, labels_to_1(labels))\n",
    "        else:\n",
    "            loss1 = 0\n",
    "            \n",
    "        if enable2:\n",
    "            loss2 = criterion2(output2, labels)\n",
    "        else:\n",
    "            loss2 = 0\n",
    "        \n",
    "        if learn_weights:\n",
    "            loss = (torch.exp(-model.weight1) * loss1 + 0.5 * model.weight1 \n",
    "                    + torch.exp(-model.weight2) * loss2 + 0.5 * model.weight2)\n",
    "        else:\n",
    "            loss = loss1 + loss2\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch {epoch}: {epoch_loss/i:.3} ({model.weight1.item()}, {model.weight2.item()})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 1: 9947/10000 (99.47%)\n",
      "Accuracy 2: 9853/10000 (98.53%)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct1 = 0\n",
    "    correct2 = 0\n",
    "    total = 0\n",
    "    \n",
    "    for i, data in enumerate(test_dataloader):\n",
    "        image, labels = data\n",
    "        \n",
    "        image = image.cuda()\n",
    "        labels = labels.cuda()\n",
    "        \n",
    "        output1, output2 = model(image)\n",
    "        preds1 = output1.argmax(dim=1)\n",
    "        preds2 = output2.argmax(dim=1)\n",
    "        \n",
    "        correct1 += (labels_to_1(labels) == preds1).sum().item()\n",
    "        correct2 += (labels == preds2).sum().item()\n",
    "        total += preds1.shape[0]\n",
    "    \n",
    "    print(f'Accuracy 1: {correct1}/{total} ({100 * correct1/total:.2f}%)')\n",
    "    print(f'Accuracy 2: {correct2}/{total} ({100 * correct2/total:.2f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just 1:\n",
    "Accuracy 1: 9868/10000 (98.68%)\n",
    "Accuracy 2: 1352/10000 (13.52%)\n",
    "\n",
    "Just 2:\n",
    "Accuracy 1: 1270/10000 (12.70%)\n",
    "Accuracy 2: 9434/10000 (94.34%)\n",
    "\n",
    "Both (equal weights):\n",
    "Accuracy 1: 9840/10000 (98.40%)\n",
    "Accuracy 2: 9457/10000 (94.57%)\n",
    "\n",
    "Both (learned weights):\n",
    "Accuracy 1: 9897/10000 (98.97%)\n",
    "Accuracy 2: 9735/10000 (97.35%)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:multitask-learning]",
   "language": "python",
   "name": "conda-env-multitask-learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
